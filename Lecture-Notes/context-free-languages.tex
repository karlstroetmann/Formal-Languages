\chapter{Context-Free Languages \label{chap:kontextfrei}}
In this chapter we present the notion of a
\href{http://en.wikipedia.org/wiki/Context-free_language}{\emph{context-free language}}.
This concept is much more powerful than the notion of a regular language.  The syntax of most modern
programming languages can be described by a context-free language.  Furthermore, checking whether a
string is a member of a context-free language structures the string into a recursive structure known
as a \blue{parse tree}.  These parse trees are the basis for understanding the meaning of a
string that is to be interpreted as a program fragment.  A program that checks whether a given
string is an element of a context-free language is called a \blue{parser}.  The task of a parser is to build
a \blue{parse tree} from a given string.  Parsing is therefore the first step in an interpreter or a compiler.
In this chapter, we first define the notion of context-free languages.  Next, we discuss parse
trees.  We conclude this chapter by introducing \blue{top down parsing}, which is one of the less complex
algorithms that are available for parsing a string.

\section{Context-Free Grammars \label{context-free}}
Context-free languages are used to describe programming languages.
Context-free languages are formal languages like regular languages, but they are much more expressive than
regular languages.
When we process a program, we not only want
decide whether the program is syntactically correct, but we also want to understand the \emph{structure}
of the program.  The process of \emph{structuring} is also referred to as \blue{parsing}
and the program that does this structuring is called a
\blue{parser}.  As input a parser usually does not receive the
text of a program, but instead a sequence of so-called \blue{terminals}, which are also called
\blue{tokens}.  These tokens are generated by a scanner, which uses regular expressions to split the program text
into single words, which we call tokens in this context.

The parser receives a sequence of tokens from the scanner and has the task of constructing a so-called
\blue{syntax tree}.  For this purpose the parser uses a
grammar which specifies how the input is to be structured.  As an example, consider parsing arithmetic
expressions.  We define the set \textsl{ArithExpr} of arithmetic expressions inductively.  
In order to correctly represent the structure of arithmetic expressions, we define
simultaneously the sets \textsl{Product} and \textsl{Factor}.  
The set \textsl{Product} contains arithmetic expressions that are
expressions representing products and quotients and the set \textsl{Factor} contains numbers and parenthesized
expressions.  The definition of these additional sets is necessary in order to get the precedences of 
the operators right.  The basic building blocks of arithmetic expressions are variables, numbers, the
operator symbols 
``\texttt{+}'', ``\texttt{-}'', ``\texttt{*}'', ``\texttt{/}'',
and the bracket symbols ``\texttt{(}'' and ``\texttt{)}''.  Based on these symbols
the inductive definition of the sets \textsl{Factor}, \textsl{Product} and
\textsl{ArithExpr} proceeds as follows:
\begin{enumerate}
\item Each number is a factor:
      \\[0.2cm]
      \hspace*{1.3cm}
      $C \in \textsl{number} \Rightarrow C \in \textsl{Factor}$.
\item Each variable is a factor:
      \\[0.2cm]
      \hspace*{1.3cm}
      $V \in \textsl{variable} \Rightarrow V \in \textsl{Factor}$.
\item If $A$ is an arithmetic expression and we enclose this expression in parentheses
      we get an expression that we can use as a factor:
      \\[0.2cm]
      \hspace*{1.3cm}
      $A \in \textsl{ArithExpr} \Rightarrow \quoted{(}A\quoted{)} \in \textsl{Factor}$. 
      \\[0.2cm] 
      A word about notation: while in the above formula $A$ is a meta-variable that stands for
      an arbitrary arithmetic expression, the strings ``\texttt{(}'' 
      and ``\texttt{)}'' are to be interpreted literally and are therefore 
      enclosed in quotation marks.  The quotation marks itself are not part of the arithmetic 
      expression but only serve the notation.
\item If $F$ is a factor, then $F$ is also a product:
      \\[0.2cm]
      \hspace*{1.3cm}
      $F \in \textsl{Factor} \Rightarrow F \in \textsl{Product}$.
\item If $P$ is a product and if $F$ is a factor, then the strings 
      $P \quoted{*} F$ and $P \quoted{/} F$ are also products:
      \\[0.2cm]
      \hspace*{1.3cm}
      $P \in \textsl{Product} \wedge F \in \textsl{Factor} \Rightarrow 
       P \squoted{*} F \in \textsl{Product} \;\wedge\; P \squoted{/} F \in \textsl{Product}$.
\item Each product is also an arithmetic expression
      \\[0.2cm]
      \hspace*{1.3cm}
      $P \in \textsl{Product} \Rightarrow P \in \textsl{ArithExpr}$.
\item If $A$ is an arithmetic expression and $P$ is a product, then
      the strings $A \quoted{+} P$ and $A \quoted{-} P$ are arithmetic expressions:
      \\[0.2cm]
      \hspace*{1.3cm}
      $A \in \textsl{ArithExpr} \wedge P \in \textsl{Product} \Rightarrow
       A \squoted{+} P \in \textsl{ArithExpr} \;\wedge\; A \squoted{-} P \in \textsl{ArithExpr}$.
\end{enumerate}

The rules given above define the sets \textsl{Factor}, \textsl{Product} and
\textsl{ArithExpr} by mutual recursion.
We can write this definition in terms of so-called \blue{grammar rules} much more
more compactly:
\begin{eqnarray*}
  \textsl{arithExpr} & \rightarrow & \textsl{arithExpr} \quoted{+} \textsl{product}  \\
  \textsl{arithExpr} & \rightarrow & \textsl{arithExpr} \quoted{-} \textsl{product}  \\
  \textsl{arithExpr} & \rightarrow & \textsl{product}                                \\[0.1cm]
  \textsl{product} & \rightarrow & \textsl{product} \quoted{*} \textsl{factor}     \\
  \textsl{product} & \rightarrow & \textsl{product} \quoted{/} \textsl{factor}     \\
  \textsl{product} & \rightarrow & \textsl{factor}                                 \\[0.1cm]
  \textsl{factor} & \rightarrow & \quoted{(} \textsl{arithExpr} \quoted{)}        \\
  \textsl{factor} & \rightarrow & \textsc{variable}                               \\
  \textsl{factor} & \rightarrow & \textsc{Number} 
\end{eqnarray*}
We refer to the expressions on the left side of a grammar rule as
\blue{syntactic variables}\index{syntactic variable} or as
\blue{non-terminals}. \index{non-terminal} All other expressions are called \blue{terminals}.
\index{terminal}
We write syntactic variables in lowercase, because that is the convention
in the parser generators \blue{\textsc{Antlr}} and \blue{\textsc{Ply}}, which we will introduce later. 
In the literature, however, it is often the other way around.  There the syntactic variables are capitalized and
the terminals are written in lower case.  Occasionally a
syntactic variable is also called a \blue{syntactic category}\index{syntactic category}.

In the example, \textsl{arithExpr}, \textsl{product} and \textsl{factor} are the 
\blue{syntactic variables}.  The remaining expressions, in our case \textsc{number},
\textsc{variable} and the characters 
``\texttt{+}'', ``\texttt{-}'', ``\texttt{*}'', ``\texttt{/}'', ``\texttt{(}'' and ``\texttt{)}''
are the \blue{terminals} or \blue{tokens}.  So these are exactly the
characters that do not appear on the left side of a grammar rule.  There are two types of terminals:
\begin{enumerate}
\item Operator symbols and separators, such as ``\texttt{/}'' and
      ``\texttt{(}''.  

      These terminals are used literally.
\item Tokens such as \textsc{number} or \textsc{variable} also have a value associated with them.
      In the case of \textsc{Number} this is a number, in the case of \textsc{Variable}
      this is a string containing the name of the variable.  We will
      always write these kinds of token with capital letters to distinguish them from the syntactic variables.
\end{enumerate}
Usually, grammar rules are rendered in a more compact notation than the one presented above.
For our example, this notation looks like this:
\begin{eqnarray*}
  \textsl{arithExpr} & \rightarrow & \textsl{arithExpr} \quoted{+} \textsl{product} \;\mid\;
                                     \textsl{arithExpr} \quoted{-} \textsl{product} \;\mid\; 
                                     \textsl{product}                                            \\
  \textsl{product} & \rightarrow & \textsl{product} \quoted{*} \textsl{factor} \;\mid\;
                                     \textsl{product} \quoted{/} \textsl{factor} \;\mid\;
                                     \textsl{factor}                                             \\
  \textsl{factor} & \rightarrow & \squoted{(}\, \textsl{arithExpr} \quoted{)} \;\mid\; 
                                     \textsc{number} \;\mid\; \textsc{variable}
\end{eqnarray*}
So here the individual alternatives of a rule are separated by the metacharacter \squoted{|}.
Following the example given above, we now present the formal definition for the notion of a
\href{http://en.wikipedia.org/wiki/Context-free_grammar}{context-free grammar}.

\begin{Definition}[Context-Free Grammar]
A \blue{context-free grammar} \index{context-free grammar} $G$ is a 4-tuple 
\[ 
   G = \langle V, T, R, S \rangle,
\]
where
\begin{enumerate}
\item $V$ is a set of names which we call \blue{syntactic variable} \index{syntactic variables}
      or \blue{non-terminals}\index{non-terminals}. 
      
      In the example above we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $V = \{ \textsl{arithExpr}, \textsl{product}, \textsl{factor} \}$.
\item $T$ is a set of names that we call \blue{terminals}\index{terminals}.  
      The sets $T$ and $V$ are disjoint, so 
      \\[0.2cm]
      \hspace*{1.3cm}
      $T \cap V = \emptyset$
      \\[0.2cm]
      holds.  In the example given above we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $T = \{ \textsc{number}, \textsc{variable}, \quoted{+}, \quoted{-}, \quoted{*}, \quoted{/}, \quoted{(}, \quoted{)} \}.$
\item $R$ is the set of \blue{grammar rules}\index{grammar rule}. Formally, a grammar rule is
      a pair of the form $\langle A, \alpha \rangle$: 
      \begin{enumerate}
      \item The first component of this pair is a syntactic variable:
            \\[0.2cm]
            \hspace*{1.3cm}
            $A \in V$.
      \item The second component is a string built from syntactic variables and 
            terminals:
            \\[0.2cm]
            \hspace*{1.3cm}
            $\alpha \in (V \cup T)^*$.
      \end{enumerate}
      Therefore, we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $R \subseteq V \times (V \cup T)^*$.
      
      If $\langle x, \alpha \rangle$ is a rule, we write this rule as
      \\[0.2cm]
      \hspace*{1.3cm}
      $x \rightarrow \alpha$.
      \\[0.2cm]
      In the example given above we have defined the first rule as
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{arithExpr} \rightarrow \textsl{arithExpr} \quoted{+} \textsl{product}$
      \\[0.2cm]
      Formally, this rule stands for the pair
      \\[0.2cm]
      \hspace*{1.3cm}
      $\bigl\langle \textsl{arithExpr}, [\textsl{arithExpr}, \squoted{+}, \textsl{product}] \bigr\rangle$. 
\item $S$ is an element of the set $V$, which we define as the \blue{start symbol}\index{start symbol}. 
      In the example above, \textsl{arithExpr} is the start symbol.
      \eox
\end{enumerate}
\end{Definition}

\subsection{Derivations}
Next, we want to determine which \blue{language} is defined by a given grammar $G$.
To do this, we first define the notion of a \blue{derivation-step}. \index{derivation-step}
Assume that
\begin{enumerate}
\item $G = \langle V, T, R, S \rangle$ is a grammar,
\item $a \in V$ is a syntactic variable,
\item $\alpha a \beta \in (V \cup T)^*$ is a string of terminals and syntactic variables,
      containing the variable $a$, and
\item $(a \rightarrow \gamma) \in R$ is a rule.
\end{enumerate}
Then the string $\alpha a \beta$ can be converted by a derivation step into the string 
$\alpha \gamma \beta$, so we replace an occurrence of the syntactic variable
$a$ by the right-hand side of the rule $a \rightarrow \gamma$.  We write this derivation step
as
\\[0.2cm]
\hspace*{1.3cm}
$\alpha a \beta \Rightarrow_G \alpha \gamma \beta$.
\\[0.2cm]
If the grammar $G$ used is clear from the context, then the index $_G$
is omitted and we write $\Rightarrow$ in place of $\Rightarrow_G$.
The transitive and reflexive closure of the relation $\Rightarrow_G$ is denoted by $\Rightarrow_G^*$
.  If we want to express that the derivation of the string $w$ from the
non-terminal $a$ consists of $n$ derivation steps
we write 
\\[0.2cm]
\hspace*{1.3cm}
$a \Rightarrow^n w$.
\\[0.2cm]  
We give an example:
\begin{eqnarray*}
\textsl{arithExpr} 
& \Rightarrow & \textsl{arithExpr} \quoted{+} \textsl{product}  \\
& \Rightarrow & \textsl{product} \quoted{+} \textsl{product}  \\
& \Rightarrow & \textsl{product} \quoted{*} \textsl{factor} \quoted{+} \textsl{product} \\
& \Rightarrow & \textsl{factor} \quoted{*} \textsl{factor} \quoted{+} \textsl{product}  \\
& \Rightarrow & \textsc{Number} \quoted{*} \textsl{factor} \quoted{+} \textsl{product}  \\
& \Rightarrow & \textsc{Number} \quoted{*} \textsc{Number} \quoted{+} \textsl{product}  \\
& \Rightarrow & \textsc{Number} \quoted{*} \textsc{Number} \quoted{+} \textsl{factor}   \\
& \Rightarrow & \textsc{Number} \quoted{*} \textsc{Number} \quoted{+} \textsc{Number}   
\end{eqnarray*}
So we have shown that
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{arithExpr} \Rightarrow^* \textsc{Number} \quoted{*} \textsc{Number} \quoted{+} \textsc{Number}$
\\[0.2cm]
or more precisely
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{arithExpr} \Rightarrow^8 \textsc{Number} \quoted{*} \textsc{Number} \quoted{+} \textsc{Number}$
\\[0.2cm]
applies.  If we replace the terminal \textsc{Number} by different numbers, we have thus
shown that the string
\\[0.2cm]
\hspace*{1.3cm}
$2 * 3 + 4$
\\[0.2cm]
is an arithmetic expression.  In general, we define the language $L(G)$ defined by a grammar $G$
as the set of all strings which on the one hand consist only of terminals and which on the
on the other hand can be derived from the start symbol $S$ of the grammar, i.e.~we have
\\[0.2cm]
\hspace*{1.3cm}
$L(G) := \bigl\{ w \in T^* \mid S \Rightarrow^* w \bigr\}$.

\exampleEng
The language \\[0.2cm]
\hspace*{1.3cm}
$L = \{ (^n )^n \mid n \in \mathbb{N} \}$ 
\\[0.2cm]
is generated by the grammar 
\\[0.2cm]
\hspace*{1.3cm}
$G = \bigl\langle \{s\}, \{ \squoted{(}, \squoted{)} \}, R, \textsl{s} \bigr\rangle$,
\\[0.2cm]
where the rules $R$ are given as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{s} \rightarrow \quoted{(} \textsl{s} \quoted{)}  \mid \varepsilon$.


\proofEng
We first show that each word $w \in L$ 
can be derived from the start symbol $\textsl{s}$:
\\[0.2cm]
\hspace*{1.3cm}
If $w \in L$, then $\textsl{s} \Rightarrow^* w$.
\\[0.2cm]
So let $w_n = (^n)^n$.  We show by induction over $n \in \mathbb{N}$ that $w_n \in L(G)$.
\begin{enumerate}
\item[B.C.:] $n=0$.
  
            We have $w_0 = \varepsilon$. Since the grammar contains the rule $s \rightarrow \varepsilon$ we
            have
            \\[0.2cm]
            \hspace*{1.3cm}
            $\textsl{s} \Rightarrow \varepsilon$,
            \\[0.2cm]
            Therefore $w_0 \in L(G)$ holds.
\item[I.S.:] $n \mapsto n + 1$. 

            The string $w_{n+1}$ is of the form $w_{n+1} = \quoted{(}w_n\quoted{)}$, where
            the string $w_n$ is of course also in $L$.
            So, according to the induction hypotheses, there is a derivation of $w_n$:
            \\[0.2cm]
            \hspace*{1.3cm}
            $\textsl{s} \Rightarrow^* w_n$.
            \\[0.2cm]
            Overall, we then have the derivation
            \\[0.2cm]
            \hspace*{1.3cm}
            $\textsl{s} \Rightarrow \quoted{(}\textsl{s}\quoted{)} \Rightarrow^* \quoted{(} w_n \quoted{)} = w_{n+1}$.
            \\[0.2cm]
            Therefore $w_{n+1} \in L(G)$.
\end{enumerate}
Next, we show that every word $w$ that can be derived from $\textsl{s}$ is an element
of the language $L$.  We perform the proof by induction on the number $n \in \mathbb{N}$ of the
derivation steps:
\begin{enumerate}
\item[B.C.:] $n = 1$.
  
            The only derivation of a string built from terminals that consists of only 
            one step is
            \\[0.2cm]
            \hspace*{1.3cm}
            $\textsl{s} \Rightarrow \varepsilon$.
            \\[0.2cm]
            Consequently, $w = \varepsilon$ must hold and because of $\varepsilon = (^0)^0 \in L$
            we have $w \in L$.
\item[I.S.:] $n \mapsto n+1$.

            If the derivative consists of more than one step, then the derivative 
            must have the following form:
            \\[0.2cm]
            \hspace*{1.3cm}
            $\textsl{s} \Rightarrow \quoted{(} \textsl{s} \quoted{)} \Rightarrow^n w$
            \\[0.2cm]
            From this it follows that
            \\[0.2cm]
            \hspace*{1.3cm}
            $w = \quoted{(}v\quoted{)}$ \quad and \quad $\textsl{s} \Rightarrow^n v$.
            \\[0.2cm]
            By i.h.~then $v \in L$ holds.  Thus there exists $k \in \mathbb{N}$ with $v = (^k)^k$.
            So we have
            \\[0.2cm]
            \hspace*{1.3cm}
            $w = \quoted{(}v\quoted{)} = ((^k)^k) = (^{k+1})^{k+1} \in L$. \qed
\end{enumerate}


\exerciseEng
We define for $w \in \Sigma^*$ and $c \in \Sigma$ the function
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{count}(w,c)$.
\\[0.2cm]
The function counts how many times the letter $c$ occurs in the word $w$.  The definition is done by induction
on the string $w$.
\begin{enumerate}
\item[B.C.:] $w = \varepsilon$.  

            We set
            \\[0.2cm]
            \hspace*{1.3cm}
            $\textsl{count}(\varepsilon, c) := 0$.
\item[I.S.:] $w = dv$ with $d \in \Sigma$ and $v \in \Sigma^*$.  

            Then $\textsl{count}(dv,c)$ is defined by case distinction:
            \\[0.2cm]
            \hspace*{1.3cm}
            $\textsl{count}(dv,c) := \left\{
             \begin{array}[c]{llr}
               \textsl{count}(v,c) + 1 & \mbox{if $c = d$};    \\
               \textsl{count}(v,c) & \mbox{falls $c \not= d$}. \\
             \end{array}\right.
            $ \eox
\end{enumerate}
We define $\Sigma = \{ \squoted{A}, \squoted{B} \}$ and define the language $L$ as the
set of words $w\in\Sigma^*$ in which the letters \squoted{A} and \squoted{B} occur with the
same frequency:
\\[0.2cm]
\hspace*{1.3cm}
$L := \bigl\{ w \in \Sigma^* \mid \textsl{count}(w,\squoted{A}) = \textsl{count}(w,\squoted{B})\bigr\}$
\\[0.2cm]
Give a grammar $G$ such that $L = L(G)$ and prove that this grammar indeed generates $L$.
\eox

\exerciseEng
Define $\Sigma := \{ \squoted{A}, \squoted{B} \}$. 
In the previous chapter, we have already defined the reversal of a string 
$w = c_1 c_2 \cdots c_{n-1} c_n \in \Sigma^*$ as the string
\\[0.2cm]
\hspace*{1.3cm}
$w^R := c_n c_{n-1} \cdots c_2 c_1$.
\\[0.2cm]
A string $w \in \Sigma^*$ is called a
\href{http://en.wikipedia.org/wiki/Palindrome}{\blue{palindrome}} \index{palindrome} if the string is identical to its
reversal, i.e.~if
\\[0.2cm]
\hspace*{1.3cm}
$w = w^R$
\\[0.2cm]
holds true.  For example, the strings 
\\[0.2cm]
\hspace*{1.3cm}
$w_1 = \mathtt{ABABA}$ \quad and \quad $w_2 = \mathtt{ABBA}$
\\[0.2cm]
are both palindromes, while the string \texttt{ABB} is not a palindrome. The 
\blue{language of palindromes} $L_\mathrm{palindrome}$ is the set of all 
strings in $\Sigma^*$ that are palindromes, i.e.~we have
\\[0.2cm]
\hspace*{1.3cm}
$L_\mathrm{palindrome} := \bigr\{ w \in \Sigma^* \mid w = w^R \bigr\}$.
\renewcommand{\labelenumi}{(\alph{enumi})}
\begin{enumerate}
\item Prove that the language $L_\mathrm{palindrome}$ is a context-free language.
\item Prove that the language $L_\mathrm{palindrome}$ is not regular.  \eox
\end{enumerate}
\renewcommand{\labelenumi}{\arabic{enumi}.}

 

% \exerciseStar
% Es sei $\Sigma := \{ \squoted{A}, \squoted{B} \}$.  Wir definieren die Menge $L$ als die
% Menge der Strings $s$, die sich \underline{nicht} in der Form $s = ww$ schreiben lassen:
% \\[0.2cm]
% \hspace*{1.3cm}
% $L = \bigl\{ s \in \Sigma^* \mid \neg(\exists w\in\Sigma^*: s = ww)\bigr\}$.
% \\[0.2cm]
% Geben Sie eine kontextfreie Grammatik $G$ an, die diese Sprache erzeugt.
% \eox

% \solution 
% Die Lösung dieser Aufgabe ist so umfangreich, dass wir unsere Überlegungen in vier Teile aufspalten.
% \vspace{0.2cm}

% \noindent
% \textbf{Vorüberlegung \texttt{I}}: String-Notationen \\
% Für einen String $s$ bezeichnen wir mit $s[i]$ den $i$-ten Buchstaben und mit
% $s[i\!:\!j]$ den Teilstring, der sich vom $i$-ten Buchstaben bis zum $j$-ten Buchstaben einschließlich
% erstreckt.  Bei der Nummerierung beginnen wir mit 1.
% Dann gilt
% \begin{enumerate}
% \item $|s[i\!:\!j]| = j - i + 1$

%       Von der Notwendigkeit, hier eine 1 zu addieren, können wir uns dadurch überzeugen, wenn wir den
%       Fall $i = j$ betrachten, denn $s[i\!:\!i]$ ist der Teilstring, der nur aus dem $i$-ten
%       Buchstaben besteht und der hat natürlich die Länge 1.
% \item $s[i\!:\!j][k] = s[i + k - 1]$.

%       Dass in diesem Fall 1 subtrahiert werden muss, sehen Sie, wenn Sie den Fall $k=1$ betrachten,
%       denn der erste Buchstabe des Teilstrings $s[i\!:\!j]$ ist natürlich der $i$-te
%       Buchstabe von $s$.
% \item Hat ein Wort $s \in \Sigma^*$ eine ungerade Länge, gilt also
%       \\[0.2cm]
%       \hspace*{1.3cm}
%       $|s| = 2 \cdot n +1$ \quad für ein $n \in \mathbb{N}$,
%       \\[0.2cm]
%       so liegt der Buchstabe $s[n + 1]$ in der Mitte von $s$.  Um dies einzusehen,
%       betrachten wir die Teilstrings $s[1:n]$ und $s[n+2:2\cdot n+1]$, die links und rechts
%       von $s[n+1]$ liegen:
%       \\[0.2cm]
%       \hspace*{1.3cm}
%       $\underbrace{s[1] \cdots s[n]}_{s[1:n]} s[n+1] \underbrace{s[n+2] \cdots s[2 \cdot n
%         +1]}_{s[n+2:2\cdot n+1]}$
%       \\[0.2cm]
%       Offenbar sind diese Teilstrings gleich lang, denn wir haben
%       \\[0.2cm]
%       \hspace*{1.3cm}
%       $|s[1:n]| = n$ \quad und \quad $|s[n+2:2\cdot n+1]| = 2 \cdot n + 1 - (n+2) + 1 = n$.
%       \\[0.2cm]
%       Also liegt der Buchstabe $s[n+1]$ tatsächlich in der Mitte von $s$.  

%       Für einen String $s$ ungerader Länge definieren wir $\hat{s}$ als den Buchstaben,
%       der in der Mitte von $s$ liegt:
%       \\[0.2cm]
%       \hspace*{1.3cm}
%       $\hat{s} := s[n+1]$ \quad falls $|s| = 2 \cdot n + 1$.
% \end{enumerate}

% \noindent
% \textbf{Vorüberlegung \texttt{II}}:
% Zunächst ist klar, dass alle Strings deren Längen ungerade sind, in der Sprache $L$ liegen,
% denn jeder String der Form $s=ww$ hat offenbar die Länge 
% \\[0.2cm]
% \hspace*{1.3cm}
% $|s| = |w| + |w| = 2\cdot |w|$
% \\[0.2cm]
% und das ist eine gerade Zahl.

% Gilt nun $s \in L$ mit $|s| = 2 \cdot n$, so lässt sich $s$ in zwei Teile $u$ und $v$ gleicher
% Länge zerlegen:
% \\[0.2cm]
% \hspace*{1.3cm}
% $s = uv \quad \mbox{mit} \quad u = s[1\!:\!n], \quad v = s[n+1\!:\!2 \cdot n]
%   \quad \mbox{und} \quad u \not= v
% $.
% \\[0.2cm]
% Aus der Ungleichung $u \not= v$ folgt, dass es mindestens einen Index $k \in \{1,\cdots,n\}$ gibt,
% so dass sich die Strings $u$ und $v$ an diesem Index unterscheiden:
% \\[0.2cm]
% \hspace*{1.3cm}
% $u[k] \not= v[k]$. 
% \\[0.2cm]
% Der Trick besteht jetzt darin, den String $s$ in zwei Teilstrings $x$ und $y$ aufzuteilen,
% von denen der eine 
% Teilstring in der Mitte den Buchstaben $u[k]$ enthält, während der andere Teilstring in der Mitte den
% Buchstaben $v[k]$ enthält.  Wir definieren
% \\[0.2cm]
% \hspace*{1.3cm}
% $x := s[1\!:\!2 \cdot k - 1] \quad \mbox{und} \quad y := s[2 \cdot k\!:\! 2 \cdot n]$.
% \\[0.2cm]
% Für die Längen von $x$ und $y$ folgt daraus
% \\[0.2cm]
% \hspace*{1.3cm}
% $|x| = 2 \cdot k - 1 \quad \mbox{und} \quad |y| = 2 \cdot (n - k) + 1$. 
% \\[0.2cm]
% Dann gilt einerseits
% \\[0.2cm]
% \hspace*{1.3cm}
% $x[k] = s[k] = u[k]$
% \\[0.2cm]
% und andererseits haben wir
% \\[0.2cm]
% \hspace*{1.3cm}
% $
% \begin{array}[t]{lcl}
%     y[n - k + 1] & = & s[2 \cdot k\!:\! 2 \cdot n][n - k + 1] \\[0.1cm]
%                  & = & s[2 \cdot k + (n - k + 1) - 1]     \\[0.1cm]
%                  & = & s[n + k]                           \\[0.1cm]
%                  & = & s[n+1\!:\! 2 \cdot n][k]               \\[0.1cm]
%                  & = & v[k]               
% \end{array}
% $
% \\[0.2cm]
% Die beiden Buchstaben $u[k]$ und $v[k]$, die dafür verantwortlich sind, dass $u$ und $v$ verschieden
% sind, befinden sich also genau in der Mitte der Strings $x$ und $y$.
% \vspace{0.3cm}

% \noindent
% \textbf{Bemerkung}: Wir haben soeben Folgendes gezeigt:  Falls $s \in L$ mit $|s| = 2 \cdot n$ ist, so lässt
% sich $s$ so in zwei Strings $x$ und $y$ aufspalten, dass die Buchstaben, die jeweils in
% der Mitte von $x$ und $y$ liegen, unterschiedlich sind:
% \\[0.2cm]
% \hspace*{1.3cm}
% $s \in L \wedge |s| = 2 \cdot n \rightarrow \exists x,y \in \Sigma^*: \bigl(
%  s = xy \wedge \hat{x} \not= \hat{y}\bigr)$.
% \vspace{0.3cm}

% \noindent
% \textbf{Vorüberlegung \texttt{III}}:
% Wir überlegen uns nun, dass auch die Umkehrung des in der letzten Bemerkung angegebenen
% Zusammenhangs gilt:  Sind $x, y \in \Sigma^*$ mit ungerader Länge und gilt 
% $\hat{x} \not= \hat{y}$, so liegt der String $xy$ in der Sprache $L$:
% \\[0.2cm]
% \hspace*{1.3cm}
% $x, y \in \Sigma^* \wedge |x| = 2 \cdot m + 1 \wedge |y| = 2 \cdot n + 1 \wedge \hat{x}
% \not= \hat{y} \rightarrow xy \in L$. \hspace*{\fill} $(*)$
% \\[0.2cm]

% \noindent
% \textbf{Beweis}: Wir definieren $s$ als die Konkatenation von $x$ und $y$, also $s := xy$.
% Für die Länge von $s$ gilt dann
% \\[0.2cm]
% \hspace*{1.3cm}
% $|s| = 2 \cdot (m + n + 1)$.
% \\[0.2cm]
% Wir werden zeigen, dass
% \\[0.2cm]
% \hspace*{1.3cm}
% $s[m+1] \not= s[(m+n+1) + (m+1)]$
% \\[0.2cm]
% gilt.  Spalten wir $s$ in zwei gleich lange Teile $u$ und $v$ auf, definieren also 
% \\[0.2cm]
% \hspace*{1.3cm}
% $u := s[1:m+n+1]$ \quad und \quad
% $v := s[m+n+2: 2\cdot(m+n+1)]$, 
% \\[0.2cm]
% so werden wir gleich sehen, dass
% \\[0.2cm]
% \hspace*{1.3cm}
% $u[m+1] = s[m+1] \not= s[(m+n+1) + (m+1)] = v[m+1]$,
% \\[0.2cm]
% gilt, woraus  $u \not= v$ und damit $s = uv \in L$ folgt. 
% \vspace{0.3cm}

% \noindent
% Es bleibt der Nachweis von  $s[m+1] \not= s[(m+n+1) + (m+1)]$ zu erledigen:
% \\[0.2cm]
% \hspace*{1.3cm}
% $
% \begin{array}[t]{lcll}
%   s[(m+n+1) + m + 1] &     = & (xy)[(m+n+1) + m + 1]  & \mbox{wegen $s = xy$} \\
%                      &     = & y[n+1]  & \mbox{denn $|x| = 2 \cdot m + 1$}    \\
%                      &     = & \hat{y} & \mbox{denn $|y| = 2 \cdot n + 1$}    \\
%                      & \not= & \hat{x}                                        \\
%                      &     = & x[m+1]  & \mbox{denn $|x| = 2 \cdot m + 1$}    \\
%                      &     = & s[m+1]  & \mbox{wegen $s = xy$}.
% \end{array}
% $
% \\[0.2cm]
% Damit ist der Beweis der Behauptung $(*)$ abgeschlossen.
% \vspace{0.2cm}

% \noindent
% \textbf{Aufstellen der Grammatik}:
% Fassen wir die letzten beiden Vorüberlegungen zusammen, so stellen wir fest, dass die
% Sprache $L$ aus genau den Wörtern besteht, die entweder 
% eine ungerade Länge haben, oder die aus Paaren von Strings ungerader Länge bestehen, die
% in der Mitte unterschiedliche Buchstaben haben:
% \\[0.2cm]
% \hspace*{1.3cm}
% $\begin{array}[t]{lcl}
%   L & =    & \bigl\{ s \in \Sigma^* \big|\; |s| \,\texttt{\%}\, 2 = 1 \bigr\}  \\[0.1cm]
%     & \cup & \bigl\{ s \in \Sigma^* \big|\; \exists x,y \in \Sigma^*: 
%                              s = xy \wedge |x| \,\texttt{\%}\, 2 = 1 
%                                     \wedge |y| \,\texttt{\%}\, 2 = 1  \wedge \hat{x} \not= \hat{y} \bigr\} 
%  \end{array}$
% \\[0.2cm]
% Damit lässt sich die Menge $L$ durch die folgende Grammatik beschreiben
% \\[0.2cm]
% \hspace*{1.3cm}
% $G = \langle \{ s, a, b, x, u \}, \{ \squoted{A}, \squoted{B} \}, R, s \rangle$,
% \\[0.2cm]
% wobei die Menge der Regeln wie folgt gegeben ist:
% \\[0.2cm]
% \hspace*{1.3cm}
% $ 
% \begin{array}[t]{lcl}
%   s & \rightarrow & u \mid a b \mid  b a \\[0.3cm]
%   a & \rightarrow & \squoted{A} \mid x a x         \\[0.3cm]
%   b & \rightarrow & \squoted{B} \mid x b x         \\[0.3cm]
%   u & \rightarrow & x \mid u x x         \\[0.3cm]
%   x & \rightarrow & \squoted{A} \mid \squoted{B}
% \end{array}$
% \\[0.2cm]
% Wir diskutieren die verschiedenen syntaktischen Variablen.
% \begin{enumerate}
% \item $L(x) = \{ \squoted{A}, \squoted{B} \}$.
% \item $L(u) = \{ w \in \Sigma^* \mid \;|w| \,\texttt{\%}\, 2 = 1 \}$,

%       denn ein String ungerader Länge hat entweder die Länge 1 oder er kann aus einem String ungerader Länge
%       durch Anfügen zweier Buchstaben erzeugt werden.
% \item $L(a) = \{ w \in \Sigma^* \mid\; \exists k \in \mathbb{N}: |w| = 2 \cdot k - 1 \wedge w[k] = \squoted{A} \}$,
  
%       denn wenn wir an einen String, bei dem der Buchstabe $\squoted{A}$ in der Mitte steht, vorne
%       und hinten jeweils einen Buchstaben anfügen, erhalten wir wieder einen String, in
%       dessen Mitte der Buchstabe $\squoted{A}$ steht
% \item $L(b) = \{ w \in \Sigma^* \mid\; \exists k \in \mathbb{N}: |w| = 2 \cdot k - 1 \wedge w[k] = \squoted{B} \}$,

%       denn die Variable $b$ ist analog zur Variablen $a$ definiert worden.  Der einzige
%       Unterschied ist der, dass nun der Buchstabe $B$ in der Mitte liegt.
% \item $
%   \begin{array}[t]{lcl}    
% L(s) & = & \quad \bigl\{ w \in \Sigma^* \big|\; |w| \,\texttt{\%}\, 2 = 1 \bigr\} \\[0.1cm] 
%      &   & \cup\; \bigl\{ w \in \Sigma^* \big|\; \exists x,y \in \Sigma^*: 
%                           w = xy \wedge |x| \,\texttt{\%}\, 2 = 1 
%                                  \wedge |y| \,\texttt{\%}\, 2 = 1  \wedge \hat{x} \not= \hat{y} \bigr\} \\[0.2cm]
%      & = & \quad \bigl\{ w \in \Sigma^* \mid \neg (\exists v \in \Sigma^*: w = vv) \bigr\}
%   \end{array}
% $
%       \\[0.2cm]
%       denn wir haben oben argumentiert, dass alle Strings der Sprache $L$ entweder eine
%       ungerade Länge haben oder in zwei Teile ungerader Länge zerlegt werden können, so dass in der Mitte
%       dieser Teile verschiedene Buchstaben stehen: Entweder steht im ersten Teil ein $\squoted{A}$
%       und im zweiten Teil steht ein $\squoted{B}$ oder es ist umgekehrt.
% \end{enumerate}
% Um die obigen Behauptungen formal zu beweisen müssten wir nun einerseits noch durch eine Induktion
% nach der Länge der Herleitung zeigen, dass die von den Grammatik-Symbolen erzeugten
% Strings tatsächlich in den oben angegebenen Mengen liegen.  Andererseits müssten wir für
% die oben angegebenen Mengen zeigen, dass sich jeder String der jeweiligen Menge auch tatsächlich mit den
% angegebenen Grammatik-Regeln erzeugen lässt.  Dieser Nachweis würde dann durch Induktion über die Länge der
% einzelnen Strings geführt werden.  Da diese Nachweise einfach sind und keine
% Überraschungen mehr bieten, verzichten wir hier darauf.
% \qed

% \remark
% Wir werden später sehen, dass das Komplement der in der letzten Aufgabe definierten Sprache $L$,
% also die Sprache
% \\[0.2cm]
% \hspace*{1.3cm}
% $L^\mathtt{c} := \Sigma^* \backslash L = \bigl\{ ww \mid  w\in\Sigma^* \bigr\}$
% \\[0.2cm]
% keine kontextfreie Sprache ist.  Damit sehen wir dann, dass die Menge der kontextfreien Sprachen
% nicht unter Komplementbildung abgeschlossen ist. \eox

\subsection{Parse Trees}
Using a grammar $G$, we can not only tell whether a given string $s$ is an
element of the language $L(G)$ generated by the grammar, we can also \blue{structure} the string
by building a \blue{parse tree}.  If a grammar is
\\[0.2cm]
\hspace*{1.3cm}
$G = \langle V, T, R, S \rangle$
\\[0.2cm]
given, a \blue{parse tree}\index{parse-tree} for this grammar is a tree satisfying the following
conditions:
\begin{enumerate}
\item The tree consists of two types of nodes:
      \begin{enumerate}
      \item The \blue{leaf nodes} are those nodes that have no outgoing edges.
      \item The \blue{inner nodes} are all those nodes that have outgoing edges.
      \end{enumerate}
\item Each \blue{inner node} is labeled with a variable.
\item Each \blue{leaf node} is labeled with a terminal or with a variable.
\item If a leaf node is labeled with a variable $a$, then the grammar contains a 
      rule of the form
      \\[0.2cm]
      \hspace*{1.3cm}
      $a \rightarrow \varepsilon$.
\item If an inner node is labeled with a variable $a$ and the children of
      of this node are labeled with the symbols $X_1$, $X_2$, $\cdots$, $X_n$, then
      the grammar $G$ contains a rule of the form 
      \\[0.2cm]
      \hspace*{1.3cm}
      $a \rightarrow X_1 X_2 \cdots X_n$.
\end{enumerate}
If we read the leave nodes of a parse tree from left to right,  they yield a word
that is derived from the grammar $G$.  Figure \ref{fig:parse-tree.dot} shows a
parse tree for the word ``\texttt{2*3+4}''.  It is derived using the grammar given above for
arithmetic expressions.

\begin{figure}[!ht]
  \centering
      \epsfig{file=Abbildungen/parse-tree.eps, scale=0.7}
  \caption{A Parse Tree for the String ``\texttt{2*3+4}''.}
  \label{fig:parse-tree.dot}
\end{figure}

Since trees of the type shown in Figure \ref{fig:parse-tree.dot} become too large very quickly
we simplify these trees using the following rules:
\begin{enumerate}
\item Is $n$ an interior node labeled with the variable $A$
      and among the children of this node there is exactly one child labeled with a terminal $o$
      then we remove this child and label the node $n$ instead with the
      terminal $o$.
\item If an inner node has only one child, we replace that node with its child.
\end{enumerate}
We call the tree obtained in this way the \blue{abstract syntax tree}.
Figure \ref{fig:abstract-syntax-tree.dot} shows the abstract syntax tree that results
from the tree in Figure \ref{fig:parse-tree.dot}.  The structure stored in this tree is exactly what we need 
to evaluate the arithmetic expression ``\texttt{2*3+4}'', because the tree shows us
the order for evaluating the operators.

\begin{figure}[!ht]
  \centering
      \epsfig{file=Abbildungen/abstract-syntax-tree.eps, scale=0.7}
  \caption{Ein abstrakter Syntax-Baum für den String ``\texttt{2*3+4}''.}
  \label{fig:abstract-syntax-tree.dot}
\end{figure}

\subsection{Ambiguous Grammars}
The grammar given at the beginning of section \ref{context-free} to describe arithmetic
expressions seems very complicated because it uses three different syntactic categories: \textsl{arithExpr},
\textsl{product}, and \textsl{factor}.  We introduce a simpler grammar $G$
which describes the same language:
\\[0.2cm]
\hspace*{1.3cm}
$G = \bigl\langle \{\textsl{expr}\}, \{ \textsc{number}, \textsc{variable}, \quoted{+}, \quoted{-}, \quoted{*}, \quoted{/}, \quoted{(}, \quoted{)} \}, R, \textsl{expr} \bigr\rangle$,
\\[0.2cm]
The rules $R$ are given as follows:
\begin{eqnarray*}
  \textsl{expr} & \rightarrow & \textsl{expr} \quoted{+} \textsl{expr}  \\
                & \mid & \textsl{expr} \quoted{-} \textsl{expr}  \\
                & \mid & \textsl{expr} \quoted{*} \textsl{expr}  \\
                & \mid & \textsl{expr} \quoted{/} \textsl{expr}  \\
                & \mid & \quoted{(} \textsl{expr} \quoted{)}     \\
                & \mid & \textsc{number}                         \\
                & \mid & \textsc{variable}                         
\end{eqnarray*}
In order to show that the string ``\texttt{2*3+4}'' is in the grammar generated by this language,
we give the following derivation:
\begin{eqnarray*}
\textsl{expr} & \Rightarrow & \textsl{expr} \quoted{+} \textsl{expr}                           \\
              & \Rightarrow & \textsl{expr} \quoted{*} \textsl{expr} \quoted{+} \textsl{expr}  \\
              & \Rightarrow & \texttt{2} \quoted{*} \textsl{expr} \quoted{+} \textsl{expr}     \\
              & \Rightarrow & \texttt{2} \quoted{*} \texttt{3} \quoted{+} \textsl{expr}        \\
              & \Rightarrow & \texttt{2} \quoted{*} \texttt{3} \quoted{+} \texttt{4}           
\end{eqnarray*}
This derivation corresponds to the abstract syntax tree shown in Fig.
\ref{fig:abstract-syntax-tree.dot}
is shown.  However, there is another derivation of the string ``\texttt{2*3+4}'' with this grammar:
\begin{eqnarray*}
\textsl{expr} & \Rightarrow & \textsl{expr} \quoted{*} \textsl{expr}                           \\
              & \Rightarrow & \textsl{expr} \quoted{*} \textsl{expr} \quoted{+} \textsl{expr}  \\
              & \Rightarrow & \texttt{2} \quoted{*} \textsl{expr} \quoted{+} \textsl{expr}     \\
              & \Rightarrow & \texttt{2} \quoted{*} \texttt{3} \quoted{+} \textsl{expr}        \\
              & \Rightarrow & \texttt{2} \quoted{*} \texttt{3} \quoted{+} \texttt{4}           
\end{eqnarray*}
This derivation corresponds to the abstract syntax tree shown in Fig.
\ref{fig:abstract-syntax-tree-prod.dot}.
In this derivation, the string ``\texttt{2*3+4}'' is apparently taken to be a product,
which contradicts the convention that the operator ``\texttt{*}'' binds stronger than the operator
``\texttt{+}''.  If we were to evaluate the string using the last syntax tree, we would
obviously get the wrong result! 
\begin{figure}[!ht]
  \centering
      \epsfig{file=Abbildungen/abstract-syntax-tree-prod.eps, scale=0.6}
  \caption{another abstract syntax tree for the string ``\texttt{2*3+4}''}
  \label{fig:abstract-syntax-tree-prod.dot}
\end{figure}
The reason for this problem is the fact that the last specified grammar is
\blue{\underline{ambi}g\underline{uous}}. \index{ambiguous grammar}
An ambiguous grammar is unsuitable for parsing.  Unfortunately, the question of whether a given
grammar is ambiguous is, in general, not
\href{http://en.wikipedia.org/wiki/Ambiguous_grammar#Recognizing_ambiguous_grammars}{decidable}:
It can be shown that this question is equivalent to the
\href{http://en.wikipedia.org/wiki/post_correspondence_problem}{\blue{Post correspondence problem}}.
Since Post's correspondence problem has been shown to be undecidable,  the
question  whether a grammar is ambiguous is also unsolvable.
Proofs of these claims can be found, for example, in the book by Hopcroft, Motwani, and Ullman \cite{hopcroft:06}. 

% \example
% Es sei $\Sigma = \{ \squoted{A}, \squoted{B} \}$.  Die Sprache $L$ enthalte alle die Wörter
% aus $\Sigma^*$, bei denen die Buchstaben \squoted{A} and \squoted{B} mit der gleichen
% Häufigkeit auftreten, es gilt also
% \\[0.2cm]
% \hspace*{1.3cm}
% $L = \bigl\{ w \in \Sigma^* \mid \textsl{count}(w, \squoted{A}) = \textsl{count}(w, \squoted{B}) \bigr\}$.
% \\[0.2cm]
% Dann wird die Sprache $L$ durch die kontextfreie Grammatik $G_1 = \langle \{s\}, \Sigma, R_1, s \rangle$ beschrieben,
% deren Regeln wie folgt gegeben sind:
% \\[0.2cm]
% \hspace*{1.3cm}
% $\textsl{s} \;\rightarrow\; \quoted{A} s \quoted{B} s \;\mid\; \quoted{B} s \quoted{A} s \;\mid\; \varepsilon$
% \\[0.2cm]
% Der Grund ist, dass ein String $w \in L$ entweder mit einem \squoted{A} oder mit einem \squoted{B}
% beginnt.  Im ersten Fall muss es zu diesem \squoted{A} ein korrespondierendes \squoted{B} geben, denn
% die Anzahl der Auftreten von \squoted{A} und \squoted{B} sind gleich.  Fassen wir den Buchstaben
% \squoted{A} wie eine öffnende Klammer auf und interpretieren den Buchstaben \squoted{B} als die zu
% \squoted{A} korrespondierende schließende Klammer, so ist klar, dass der String, der zwischen diesen
% beiden Auftreten von \squoted{A} und \squoted{B} liegt, ebenfalls gleich viele Auftreten von
% \squoted{A} wie von \squoted{B} hat.  Genauso muss dies dann für den Rest des Strings gelten, der nach
% dem \squoted{B} folgt.  Diese Überlegung erklärt die Regel
% \\[0.2cm]
% \hspace*{1.3cm}
% $\textsl{s} \;\rightarrow\; \quoted{A} s \quoted{B} s$
% \\[0.2cm]
% Die Regel
% \\[0.2cm]
% \hspace*{1.3cm}
% $\textsl{s} \;\rightarrow\; \quoted{B} s \quoted{A} s$
% \\[0.2cm]
% lässt sich in analoger Weise erklären,  wenn wir den Buchstaben \squoted{B} als öffnende Klammer und
% \squoted{A} als schließende Klammer interpretieren. 

% Diese Grammatik ist allerdings mehrdeutig: Betrachten wir beispielsweise den String 
% ``\texttt{ABAB}'', so stellen wir fest, dass sich dieser prinzipiell auf zwei Arten ableiten lässt:
% \begin{eqnarray*}
%   s & \Rightarrow &\quoted{A} s \quoted{B} s                       \\
%     & \Rightarrow &\quoted{A} \quoted{B} s                         \\
%     & \Rightarrow &\quoted{A} \quoted{B}\quoted{A} s \quoted{B} s \\
%     & \Rightarrow &\quoted{A} \quoted{B}\quoted{A} \quoted{B} s   \\
%     & \Rightarrow &\quoted{A} \quoted{B}\quoted{A} \quoted{B} 
% \end{eqnarray*}
% Eine andere Ableitung desselben Strings ergibt sich, wenn wir im zweiten Ableitungs-Schritt nicht das erste
% $s$ durch $\varepsilon$ ersetzen sondern stattdessen das zweite $s$ durch $\varepsilon$ ersetzen:
% \begin{eqnarray*}
%   s & \Rightarrow &\quoted{A} s \quoted{B} s                       \\
%     & \Rightarrow &\quoted{A} s \quoted{B}                         \\
%     & \Rightarrow &\quoted{A} \quoted{B} s\quoted{A} s \quoted{B} \\
%     & \Rightarrow &\quoted{A} \quoted{B}\quoted{A} s \quoted{B}   \\
%     & \Rightarrow &\quoted{A} \quoted{B}\quoted{A} \quoted{B}     \\
% \end{eqnarray*}
% Abbildung \ref{fig:ambiguous-a.dot} zeigt die Parse-Bäume, die sich aus den beiden Ableitungen ergeben.
% Wir können erkennen, dass die Struktur dieser Bäume unterschiedlich ist:  Im ersten Fall gehört das erste
% ``\texttt{A}'' zu dem ersten ``\texttt{B}'', im zweiten Fall gehört das erste ``\texttt{A}'' zu dem letzten
% ``\texttt{B}''.

% \begin{figure}[!ht]
%       \epsfig{file=Abbildungen/ambiguous-a.eps, scale=0.6}
% \quad
%       \epsfig{file=Abbildungen/ambiguous-b.eps, scale=0.6}
%   \caption{Zwei strukturell verschiedene Parse-Bäume für den String ``\texttt{ABAB}''.}
%   \label{fig:ambiguous-a.dot}
% \end{figure}

% Wir definieren nun eine  kontextfreie Grammatik $G_2 = \langle \{s, u, v, x, y\}, \Sigma, R_2, s \rangle$,
% deren Regeln wie folgt gegeben sind:
% \hspace*{1.3cm}
% \begin{eqnarray*}
% \textsl{s} & \rightarrow & \textsl{u}\, \textsl{s} \;\mid\; \textsl{v}\, \textsl{s} \;\mid\; \varepsilon \\[0.2cm]
% \textsl{u} & \rightarrow &\quoted{A} \textsl{x} \quoted{B}                \\[0.2cm]
% \textsl{v} & \rightarrow & \quoted{B} \textsl{y} \quoted{A}                \\[0.2cm]
% \textsl{x} & \rightarrow & \textsl{u}\, \textsl{x} \;\mid\; \varepsilon \\[0.2cm]
% \textsl{y} & \rightarrow & \textsl{v}\,  \textsl{y} \;\mid\; \varepsilon          
% \end{eqnarray*}
% Um die Sprachen, die von den einzelnen Variablen erzeugt werden, klarer beschreiben zu
% können, definieren wir für zwei Strings $\sigma$ und $\omega$ die Relation $\sigma \preceq \omega$ (lese: $\sigma$ ist ein
% Präfix von $\omega$) wie folgt:
% \\[0.2cm]
% \hspace*{1.3cm}
% $\sigma \preceq \omega \quad \stackrel{\rm{def}}{\Longleftrightarrow}\quad \exists \tau \in \Sigma^*: \sigma \tau = \omega$
% \\[0.2cm]
% Sodann bemerken wir, dass von den syntaktischen Variablen $x$ und $y$ die folgenden
% Sprachen erzeugt werden:
% \\[0.2cm]
% \hspace*{1.3cm} 
% $L(x) = \bigl\{ \omega \in L \mid \forall \sigma \preceq \omega : 
%                   \textsl{count}(\sigma,\squoted{B}) \leq \textsl{count}(\sigma,\squoted{A}) \bigr\}$
% \quad und \\[0.2cm]
% \hspace*{1.3cm}
% $L(y) = \bigl\{ \omega \in L \mid \forall \sigma \preceq \omega : 
%                   \textsl{count}(\sigma, \squoted{A}) \leq \textsl{count}(\sigma, \squoted{B}) \bigr\}$.
% \\[0.2cm]
% Ist $w \in L(x)$, so gibt es zu jedem Auftreten des Buchstabens ``\texttt{B}'' in dem String $w$ ein
% dazu korrespondierendes Auftreten des Buchstabens ``\texttt{A}'', das dem Auftreten des Buchstabens
% ``\texttt{B}'' vorangeht.  Würden wir den Buchstaben
% ``\texttt{A}'' durch eine öffnende Klammer und den Buchstaben ``\texttt{B}'' durch eine schließende
% Klammer ersetzen, so wird also niemals eine Klammer geschlossen, die nicht vorher geöffnet wurde.
% Damit ist klar, dass in einem String der Form
% \\[0.2cm]
% \hspace*{1.3cm}
% ``\texttt{A}'' $w$ ``\texttt{B}'' \quad mit $w \in L(x)$ 
% \\[0.2cm]
% das zu dem ersten ``\texttt{A}'' korrespondierende ``\texttt{B}'' nur das letzte ``\texttt{B}'' sein kann.
% Analog können wir sehen, dass in einem String der Form
% \\[0.2cm]
% \hspace*{1.3cm}
% ``\texttt{B}'' $w$ ``\texttt{A}'' \quad mit $w \in L(y)$ 
% \\[0.2cm]
% das zu dem ersten ``\texttt{B}'' korrespondierende ``\texttt{A}'' nur das letzte ``\texttt{A}'' sein kann.

% Ein String der Sprache $L$ fängt nun entweder mit ``\texttt{A}'' oder mit ``\texttt{B}''
% an.  Im ersten Fall interpretieren wir das ``\texttt{A}'' als öffnende Klammer und 
% das ``\texttt{B}'' als schließende Klammer und suchen nun das ``\texttt{B}'', das dem
% ``\texttt{A}'' am Anfang des Strings zugeordnet ist.  Der String, der mit dem
% ``\texttt{A}'' anfängt und dem ``\texttt{B}'' endet, liegt in der Sprache $L(u)$.
% Auf dieses ``\texttt{B}'' kann dann noch ein weiterer Teilstring folgen, der
% gleich viele ``\texttt{A}''s und ``\texttt{B}''s enthält.  Ein solcher Teilstring liegt
% offensichtlich ebenfalls in der Sprache $L$ und kann daher von $s$ mittels der Regel
% \\[0.2cm]
% \hspace*{1.3cm}
% $\textsl{s} \rightarrow \textsl{u}\, \textsl{s}$
% \\[0.2cm]
% erzeugt werden.
% Im zweiten Fall fängt der String mit einem ``\texttt{B}'' an.  Dieser Fall ist
% analog zum ersten Fall.    \qed
% \vspace*{0.3cm}

% In dem obigen Beispiel hatten wir Glück und konnten eine Grammatik finden, mit der sich
% die Sprache eindeutig parsen lässt.  Es  gibt allerdings auch kontextfreie Sprachen, die 
% \href{http://en.wikipedia.org/wiki/Ambiguous_grammar#Inherently_ambiguous_languages}{inhärent mehrdeutig}
% \index{inhärent mehrdeutig}
% sind: Es lässt sich beispielsweise zeigen, dass für das Alphabet 
% $\Sigma =  \{ \squoted{A}, \squoted{B}, \squoted{C}, \squoted{D} \}$
% die Sprache
% \\[0.2cm]
% \hspace*{1.3cm}
% $L =  \bigl\{ \mathtt{A}^m \mathtt{B}^m \mathtt{C}^n \mathtt{D}^n \mid m, n \in \mathbb{N} \bigr\}
%  \cup \bigl\{ \mathtt{A}^m \mathtt{B}^n \mathtt{C}^n \mathtt{D}^m \mid m, n \in \mathbb{N} \bigr\}
% $
% \\[0.2cm]
% kontextfrei ist, aber jede Grammatik $G$ mit der Eigenschaft $L = L(G)$ ist
% notwendigerweise mehrdeutig.  Das Problem ist, dass für gewisse große Zahlen $n\in \mathbb{N}$ ein
% String der Form 
% \\[0.2cm]
% \hspace*{1.3cm}
% $\mathtt{A}^n \mathtt{B}^n \mathtt{C}^n \mathtt{D}^n$
% \\[0.2cm]
% immer zwei strukturell verschiedene Parse-Bäume besitzen muss.  Ein Beweis dieser Behaupung
% findet sich in der ersten Auflage des Buchs von  Hopcroft und Ullman auf Seite 100 \cite{hopcroft:79}.   

\section{Top-Down Parser}
In this section, we present a method that can be used to conveniently parse a whole range of
grammars.  The basic idea is simple: In order to parse a string $w$ using
a grammar rule of the form
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{a} \rightarrow \textsl{X}_1 \textsl{X}_2 \cdots \textsl{X}_n$
\\[0.2cm]
we try to parse an $X_1$ first.  In doing so, we decompose the string $w$ into the
form
$w = w_1 r_1$ such that $w_1 \in L(X_1)$ holds.  Then we try to find an $X_2$ in the residual string
$r_1$, thus decomposing $r_1$ as $r_1 = w_2 r_2$ where
$w_2 \in L(X_2)$ holds.  Continuing this process, we end up with the string $w$ being split as
\\[0.2cm]
\hspace*{1.3cm}
$w = w_1 w_2 \cdots w_n$ \quad with $w_i \in L(X_i)$ for all $i=1,\cdots,n$.
\\[0.2cm]
Unfortunately, this procedure does not work when the grammar is
\blue{left-recursive}\index{left-recursive}, that is, a rule has the form
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{a} \rightarrow \textsl{a}\, \beta$
\\[0.2cm]
because then to parse an $\textsl{a}$ we would immediately try again to parse an $a$ and thus we would be stuck in an infinite loop.
There are two ways to deal with this kind of problem:
\begin{enumerate}[(a)]
\item We can rewrite the grammar so that it no longer is left-recursive.
\item A simpler method is to extend the notion of a context-free grammar.
      We will use the notion of a so called \blue{extended Backus Naur form} grammar
      (abbreviated as \textsc{Ebnf}-grammar).   Theoretically, the expressive power of
      \textsc{Ebnf} grammars is the same as the expressive power of context-free grammars.
      In practice, however, it turns out that the construction of top-down parsers for
      \textsc{Ebnf} grammars is easier, because in an \textsc{Ebnf} grammar the left recursion can be replaced
      by iteration. 
\end{enumerate}
In the rest of this chapter we will discuss these two procedures in more detail using the grammar for
arithmetic expressions as an example.  


\subsection{Rewriting a  Grammar to Eliminate Left Recursion \label{left-recursion}}
In the following grammar, $a$ is a syntactic variable and the Greek letters $\beta$ and
$\gamma$ stand for any strings consisting of syntactic variables and tokens.
If the syntactic variable $a$ is defined by the two rules
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
a & \rightarrow & a \beta \\
  & \mid & \gamma
\end{array}
$
\\[0.2cm]
then a derivation of $a$, where we always replace the syntactic variable $a$ first, has the form 
\\[0.2cm]
\hspace*{1.3cm}
$a \Rightarrow a \beta \Rightarrow a \beta \beta \Rightarrow a \beta \beta \beta
 \Rightarrow \cdots \Rightarrow a \beta^n \Rightarrow \gamma \beta^n$.
\\[0.2cm]
Thus we see that the language $L(a)$ described by the syntactic variable $a$ consists of all the
strings that can be derived from the expression $\gamma \beta^n$:
\\[0.2cm]
\hspace*{1.3cm}
$L(a) = \bigl\{ w \in \Sigma^* \mid \exists n \in \mathbb{N}: \gamma \beta^n \Rightarrow^* w \bigr\}$.
\\[0.2cm]
This language can also be described by the following rules for $a$:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
a & \rightarrow & \gamma b \\[0.2cm]
b & \rightarrow & \beta b \\
  & \mid & \varepsilon 
\end{array}
$
\\[0.2cm]
Here we have introduced the auxiliary variable $b$.  The derivations resulting from the nonterminal $b$
have the form
\\[0.2cm]
\hspace*{1.3cm} $b \Rightarrow \beta b \Rightarrow \beta \beta b \Rightarrow \cdots \Rightarrow
\beta^n b \Rightarrow \beta^n$.
\\[0.2cm]
Hence the non-terminal $b$ describes the language
\\[0.2cm]
\hspace*{1.3cm} $L(b) = \bigl\{ w \in \Sigma \mid \exists n \in \mathbb{N}: \beta^n \Rightarrow w
\bigr\}$.
\\[0.2cm]
Thus it is clear that with the grammar given above we have
\\[0.2cm]
\hspace*{1.3cm} $L(a) = \bigl\{ w \in \Sigma^* \mid \exists n \in \mathbb{N}: \gamma \beta^n
\Rightarrow^* w \bigr\}$.
\\[0.2cm]
To remove the left recursion from the grammar shown in Figure \ref{fig:Expr} on page \pageref{fig:Expr},
we need to generalize the example given above.  We now consider
the general case and assume that a non-terminal $a$ is defined by rules of the following form:
\\[0.2cm]
\hspace*{1.3cm} $
\begin{array}[t]{lcl}
a & \rightarrow & a \beta_1 \\
  & \mid & a \beta_2 \\
  & \vdots & \vdots \\
  & \mid & a \beta_k \\[0.2cm]
  & \mid & \gamma_1 \\
  & \vdots & \vdots \\
  & \mid & \gamma_l
\end{array}
$
\\[0.2cm]
We can reduce this case to the first case by introducing two auxiliary variables $b$ and $c$:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
a & \rightarrow & a b \mid c \\[0.2cm]
b & \rightarrow & \beta_1 \mid \cdots \mid \beta_k \\[0.2cm]
c & \rightarrow & \gamma_1 \mid \cdots \mid \gamma_l
\end{array}
$
\\[0.2cm]
Then we can rewrite the grammar by introducing a new auxiliary variable, let's call it $l$
for list, and get
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
a & \rightarrow & c\;l \\[0.2cm]
l & \rightarrow & b\;l \mid \varepsilon.  
\end{array}
$
\\[0.2cm]
The auxiliary variables $b$ and $c$ can now be eliminated again.  Then we get the following
grammar: 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
a & \rightarrow & \gamma_1\;l \;\mid\; \gamma_2\;l \;\mid\; \cdots \;\mid\; \gamma_l\;l \\[0.2cm]
l & \rightarrow & \beta_1 \;l \;\mid\; \beta_2 \;l \;\mid\; \cdots \;\mid\; \beta_k \;l \;\mid\; \varepsilon
\end{array}
$
\begin{figure}[htbp]
  \begin{center}    
  \framebox{
  \framebox{
  \begin{minipage}[t]{8cm}
  \begin{eqnarray*}
  \textsl{expr} & \rightarrow & \;\textsl{expr} \quoted{+} \textsl{product}  \\
                   & \mid & \;\textsl{expr} \quoted{-} \textsl{product}  \\
                   & \mid & \;\textsl{product}                           \\[0.2cm]
  \textsl{product} & \rightarrow & \;\textsl{product} \quoted{*} \textsl{factor} \\
                   & \mid & \;\textsl{product} \quoted{/} \textsl{factor}\\
                   & \mid & \;\textsl{factor}                            \\[0.2cm]
  \textsl{factor} & \rightarrow & \quoted{(} \textsl{expr} \quoted{)}        \\
                   & \mid & \;\textsc{number} 
  \end{eqnarray*}
  \vspace*{-0.5cm}
  \end{minipage}}}
  \end{center}
  \caption{left-recursive grammar for arithmetic expressions}
  \label{fig:Expr}
\end{figure}
\vspace*{0.3cm}


\noindent
If we apply this procedure to the grammar for arithmetic expressions shown in Figure \ref{fig:Expr}, we obtain
the grammar shown in Figure \ref{fig:Expr2}. The variables \textsl{exprRest} and \textsl{productRest} can be
interpreted as follows:

\begin{figure}[htbp]
  \begin{center}    
  \framebox{
  \framebox{
  \begin{minipage}[t]{9cm}
  \begin{eqnarray*}
  \textsl{expr}        & \rightarrow & \;\textsl{product}\;\;\textsl{exprRest}            \\[0.2cm]
  \textsl{exprRest}    & \rightarrow & \quoted{+} \textsl{product}\;\;\textsl{exprRest}   \\
                       & \mid        & \quoted{-} \textsl{product}\;\;\textsl{exprRest}   \\
                       & \mid        & \;\varepsilon                                      \\[0.2cm]
  \textsl{product}     & \rightarrow & \;\textsl{factor}\;\;\textsl{productRest}          \\[0.2cm]
  \textsl{productRest} & \rightarrow & \quoted{*} \textsl{factor}\;\;\textsl{productRest} \\
                       & \mid        & \quoted{/} \textsl{factor}\;\;\textsl{productRest} \\
                       & \mid        & \;\varepsilon                                      \\[0.2cm]
  \textsl{factor}      & \rightarrow & \quoted{(} \textsl{expr} \quoted{)}                \\
                       & \mid        & \;\textsc{Number} 
  \end{eqnarray*}
  \vspace*{-0.5cm}
  \end{minipage}}}
  \end{center}
  \caption{Grammatik für arithmetische Ausdrücke ohne Links-Rekursion.}
  \label{fig:Expr2}
\end{figure}

\begin{enumerate}
\item \textsl{exprRest} describes a list of the form.
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{op} \;\textsl{product} \;\cdots \;\textsl{op}\; \textsl{product}$,
      \\[0.2cm]
      where $\textsl{op} \in \{ \quoted{+}, \quoted{-} \}$.
\item \textsl{productRest} describes a list of the form
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{op} \;\textsl{factor} \;\cdots \;\textsl{op} \;\textsl{factor}$,
      \\[0.2cm]
      where $\textsl{op} \in \{ \quoted{*}, \quoted{/} \}$ holds. 
\end{enumerate}
\pagebreak

\exerciseEng \label{exercise:regexp}
\begin{enumerate}[(a)]
\item The following grammar describes regular expressions:
      \begin{center}    
          \framebox{
            \begin{minipage}[t]{9cm}
              \begin{eqnarray*}
                \textsl{regExp} & \rightarrow & \;\textsl{regExp} \quoted{+} \textsl{regExp}    \\
                                & \mid & \;\textsl{regExp} \;\textsl{regExp}           \\
                                & \mid & \;\textsl{regExp}\quoted{*}                     \\
                                & \mid & \quoted{(} \textsl{regExp} \quoted{)}           \\
                                & \mid & \;\textsc{letter}                               
              \end{eqnarray*}
              \vspace*{-0.5cm}
            \end{minipage}}
      \end{center}
      This grammar uses only the syntactic variable $\{ \textsl{regExp} \}$ and the following 
      Terminals
      \\[0.2cm]
      \hspace*{1.3cm}
      $\{\squoted{+}, \squoted{*}, \squoted{(}, \squoted{)}, \textsc{letter}\}$.
      \\[0.2cm]
      Since the grammar is ambiguous, this grammar is unsuitable for parsing.
      Transform this grammar into an unambiguous grammar where the
      postfix operator ``\texttt{*}'' binds more strongly than the concatenation of two regular
      expressions, while the ``\texttt{+}'' operator binds weaker than concatenation. 
      Use the grammar for arithmetic expressions as a guide and introduce suitable new syntactic
      variables.
\item Remove the left recursion from the grammar created in part (a) of this task.
      \eox
\end{enumerate}


\subsection{Implementing a Top Down Parser in \textsl{Python}}


\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.0cm,
                xrightmargin  = 0.0cm
              ]{python3}
    import re
    
    def tokenize(s):
        lexSpec = r'''([ \t]+)        |  # blanks and tabs
                      ([1-9][0-9]*|0) |  # number
                      ([()])          |  # parentheses 
                      ([-+*/])        |  # arithmetical operators
                      (.)                # unrecognized character
                   '''
        tokenList = re.findall(lexSpec, s, re.VERBOSE)
        result    = []
        for ws, number, parenthesis, operator, error in tokenList:
            if ws:        # skip blanks and tabs
                pass
            if number:
                result += [ number ]
            if parenthesis:
                result += [ parenthesis ]
            if operator:
                result += [ operator ]
            if error:
                result += [ f'ERROR({error})']
        return result
\end{minted}
\vspace*{-0.3cm}
\caption{A scanner for arithmetic expressions.}
\label{fig:Top-Down-Parser:scanner.ipynb}
\end{figure}


\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.0cm,
                xrightmargin  = 0.0cm
              ]{python3}
    def parse(s):
         TL           = tokenize(s)
         result, Rest = parseExpr(TL)
         assert Rest == [], f'Parse Error: could not parse {TL}'
         return result
    
    def parseExpr(TL):
        product, Rest = parseProduct(TL)
        return parseExprRest(product, Rest)
    
    def parseExprRest(Sum, TL):
        if TL == []:
            return Sum, []
        elif TL[0] == '+':
            product, Rest = parseProduct(TL[1:])
            return parseExprRest(Sum + product, Rest)
        elif TL[0] == '-':
            product, Rest = parseProduct(TL[1:])
            return parseExprRest(Sum - product, Rest)
        else:
            return Sum, TL
    
    def parseProduct(TL):
        factor, Rest = parseFactor(TL)
        return parseProductRest(factor, Rest)
    
    def parseProductRest(product, TL):
        if TL == []:
            return product, []
        elif TL[0] == '*': 
            factor, Rest = parseFactor(TL[1:])
            return parseProductRest(product * factor, Rest)
        elif TL[0] == '/':
            factor, Rest = parseFactor(TL[1:])
            return parseProductRest(product / factor, Rest)
        else:
            return product, TL
    
    def parseFactor(TL):
        if TL[0] == '(': 
            expr, Rest = parseExpr(TL[1:])
            assert Rest[0] == ')', 'Parse Error: expected ")"'
            return expr, Rest[1:]
        else: 
            return int(TL[0]), TL[1:]
\end{minted}
\vspace*{-0.3cm}
\caption{A top down parser for arithmetic expressions.}
\label{fig:Top-Down-Parser.ipynb}
\end{figure}


\noindent
Now we are ready to implement a parser for recognizing arithmetic expressions.
We will use the grammar that is shown in Figure \ref{fig:Expr2} on page \pageref{fig:Expr2}.
Before we can implement the parser, we need a scanner.  We will use a hand-coded scanner that is shown in
Figure \ref{fig:Top-Down-Parser:scanner.ipynb} on page \pageref{fig:Top-Down-Parser:scanner.ipynb}.
The function \texttt{tokenize} implemented in this scanner receives a string \texttt{s} as argument and returns
a list of tokens.  The string \texttt{s} is supposed to represent an arithmetical expression. 
In order to understand the implementation, you need to know the following:
\begin{enumerate}[(a)]
\item We need to set the flag \texttt{re.VERBOSE} in our call of the function \texttt{findall}
      below because otherwise we are not able to format the regular expression \texttt{lexSpec} the way 
      we have done it.
\item The regular expression \texttt{lexSpec} contains 5 parenthesized groups.  Therefore,
      \texttt{findall} returns a list of 5-tuples where the 5 components correspond to the 5
      groups of the regular expression.  As the 5 groups are non-overlapping, exactly one of the 5 components
      will be a non-empty string.
\end{enumerate}
Figure \ref{fig:Top-Down-Parser.ipynb} on page
\pageref{fig:Top-Down-Parser.ipynb} shows an implementation of a recursive descent parser in
\textsc{Python}. 
\begin{enumerate}[(a)]
\item The main function is the function \texttt{parse}. This function takes a string $s$
      representing an arithmetic expression.  This string is tokenized using the 
      function \texttt{tokenize}.  The function \texttt{tokenize} turns a
      string into a list of tokens.  For example, the expression
      \\[0.2cm]
      \hspace*{1.3cm}
      \verb|tokenize('(1 + 2) * 3')|
      \\[0.2cm]
      returns the result
      \\[0.2cm]
      \hspace*{1.3cm}
      \verb|['(', 1, '+', 2, ')', '*', 3]|.
      \\[0.2cm]
      This list of tokens is then parsed by the function \texttt{parseExpr}.
      That function returns a pair: 
      \begin{enumerate}
      \item The first  component is the value of the arithmetic expression.
      \item The second component is the list of those tokens that have not been consumed
            when parsing the expression.  Of course, on a successful parse this list
            should be empty.
      \end{enumerate}
\item The function \texttt{parseExpr} implements the grammar rule
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{expr} \;\rightarrow\;\textsl{product}\;\;\textsl{exprRest}$. 
      \\[0.2cm]
      It takes a token list \texttt{TL} as input.  It will return a pair of the form
      \\[0.2cm]
      \hspace*{1.3cm}
      \texttt{($v$, Rest)},
      \\[0.2cm]
      where $v$ is the value of the arithmetic expression that has been parsed, while
      \texttt{Rest} is the list of the remaining tokens.  For example, the expression
      \\[0.2cm]
      \hspace*{1.3cm}
      \verb|parseExpr(['(', 1, '+', 2, ')', '*', 3, ')', '*', 2])|
      \\[0.2cm]
      returns the result
      \\[0.2cm]
      \hspace*{1.3cm}
      \verb|[9, [')', '*', 2]]|.
      \\[0.2cm]
      Here, the part \verb|['(', 1, '+', 2, ')', '*', 3]| has been parsed and evaluated as
      the number $9$ and \verb|[')', '*', 2]| is the list of tokens that have not yet been
      processed.

      In order to parse an arithmetic expression, the function first parses a
      \textsl{product} and then it tries to parse the remaining tokens as an
      \textsl{exprRest}.   The function \texttt{parseExprRest} that is used to parse an
      \textsl{exprRest} needs two arguments:
      \begin{enumerate}
      \item The first argument is the value of the product that has been parsed 
            by the function \texttt{parseProduct}.
      \item The second argument is the list of tokens that can be used.
      \end{enumerate}
      To understand the mechanics of \texttt{parseExpr}, consider the evaluation of
      \\[0.2cm]
      \hspace*{1.3cm}
      \verb|[1, '*', 2, '+', 3]|.
      \\[0.2cm]
      Here, the function \texttt{parseProduct} will return the result
      \\[0.2cm]
      \hspace*{1.3cm}
      \verb|(2, ['+', 3])|,
      \\[0.2cm]
      where $2$ is the result of parsing and evaluating the token list \verb|[1, '*', 2]|, while
      \verb|['+', 3]| is the part of the input token list that is not used by
      \texttt{parseProduct}.  Next, the list \verb|['+', 3]| needs to be parsed as 
      the rest of an expression and then $3$ needs to be added to $2$.      
\item The function \texttt{parseExprRest} takes a number and a list of tokens.
      It implements the following grammar rules:
      \hspace*{1.3cm}
      \begin{eqnarray*}
        \textsl{exprRest} & \rightarrow & \quoted{+} \textsl{product}\;\;\textsl{exprRest} \\
                          & \mid        & \quoted{-} \textsl{product}\;\;\textsl{exprRest} \\
                          & \mid        & \;\varepsilon                                    
      \end{eqnarray*}
      Therefore, it checks whether the first token is either \squoted{+} or \squoted{-}.
      If the token is \squoted{+}, it parses a \textsl{product}, adds the result of this 
      product to the \texttt{sum} of values parsed already and proceeds to parse the rest
      of the tokens.  

      The case that the first token is \squoted{-} is similar to the previous case.
      If the next token is neither \squoted{+} nor \squoted{-}, then it could be either the
      token \squoted{)} or else it might be the case that the list of tokens is already
      exhausted.  In either case, the rule
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{exprRest} \;\rightarrow\; \varepsilon$
      \\[0.2cm]
      is used.  Therefore, in that case we have not consumed any tokens and hence
      the input argument is already the result.
\item The function \texttt{parseProduct} implements the rule
      \\[0.2cm]
      \hspace*{1.3cm}
      $\textsl{product} \;\rightarrow\; \textsl{factor} \;\; \textsl{exprRest}$.
      \\[0.2cm]
      The implementation is similar to the implementation of \textsl{parseExpr}.
\item The function \texttt{parseProductRest} implements the rules
      \begin{eqnarray*}
      \textsl{productRest} & \rightarrow & \quoted{*} \textsl{factor}\;\;\textsl{productRest} \\
                       & \mid        & \quoted{/} \textsl{factor}\;\;\textsl{productRest}     \\
                       & \mid        & \;\varepsilon                                      
      \end{eqnarray*}
      The implementation is similar to the implementation of \textsl{parseExprRest}.
\item The function \texttt{parseFactor} implements the rules
      \begin{eqnarray*}
      \textsl{factor} & \rightarrow & \quoted{(} \textsl{expr} \quoted{)} \\
                      & \mid        & \;\textsc{Number} 
      \end{eqnarray*}
      Therefore, we first check whether the next token is \squoted{(} because in that case,
      we have to use the first grammar rule, otherwise we use the second.
\end{enumerate}
The parser shown in Figure \ref{fig:Top-Down-Parser.ipynb} does not contain any error handling. 
Appropriate error handling will be discussed once we have covered the theory of top-down parsing.

\exerciseEng
In Exercise 21 on page \pageref{exercise:regexp} you have developed a grammar for regular expressions that does
not contain left recursion.  Implement a top down parser for this grammar.  The resulting grammar should return
a nested tuple that represents a regular expression.
\eox

\subsection{Implementing a Recursive Descent Parser that Uses an \textsc{EBNF} Grammar}
The previous solution to parse an arithmetical expression was not completely
satisfying:  The reason is that we did not really fix the problem of left recursion but rather cured the
symptoms.  The underlying reason for left recursion is that context free grammars are not that convenient to
describe the structure of programming languages since a description of this structure needs both
recursion and iteration, but context-free grammars provide no direct way to describe iteration.
Rather, they simulate iteration via recursion.  Let us therefore extend the power of context-free
languages 
slightly by admitting regular expression on the right hand side of grammar rules.  
These new type of grammars are known as
\href{http://en.wikipedia.org/wiki/Extended_Backus_Naur_Form}{\emph{extended Backus Naur form}}
\index{\textsc{Ebnf}-Grammar} grammars, which 
is abbreviated as \textsc{Ebnf} grammars.  An \textsc{Ebnf} grammar admits the operators
\squoted{*}, \squoted{?}, and \squoted{+} on the right hand side of a grammar
rule.  The meaning of these operators is the same as when these operators are used in 
the regular expressions of the programming language \textsl{Python}.
Furthermore, the right hand side of a grammar rule can be structured using parentheses.

\begin{figure}[htbp]
  \begin{center}    
  \framebox{
  \framebox{
  \begin{minipage}[t]{9cm}

  \begin{eqnarray*}
  \mathrm{expr}    & \rightarrow & \mathrm{product}\;\;\bigl((\texttt{'+'}\;|\;\texttt{'-'})\;\; \mathrm{product}\bigr)^* \\[0.2cm]
  \mathrm{product} & \rightarrow & \mathrm{factor} \;\;\bigl((\texttt{'*'}\;|\;\texttt{'/'})\;\; \mathrm{factor}\bigr)^*  \\[0.2cm]   
  \mathrm{factor}  & \rightarrow & \texttt{'('} \;\;\mathrm{expr} \;\;\texttt{')'}                             \\
                   & \mid        & \texttt{NUMBER} 
  \end{eqnarray*}
  \vspace*{-0.5cm}

  \end{minipage} \hspace*{1.cm}}}
  \end{center}
  \caption{\textsc{Ebnf} grammar for arithmetical expressions.}
  \label{fig:arith-expr-ebnf}
\end{figure}

It can be shown that the languages described by \textsc{Ebnf} grammars are still context-free
languages.  Therefore, these operators do not change the expressive power of context-free 
grammars. 
However, it is often much more \underline{convenient} to describe a language using an \textsc{Ebnf}
grammar rather than using a context-free grammar.  Figure \ref{fig:arith-expr-ebnf}
displays an \textsc{Ebnf} grammar for arithmetical expressions.  

Obviously, the grammar in Figure \ref{fig:arith-expr-ebnf}  is
more concise than the context-free grammar shown in Figure \ref{fig:Expr2} on page \pageref{fig:Expr2}.
For example, the first rule clearly expresses that an arithmetical expression is a list of
products that are separated by the operators \squoted{+} and \squoted{-}.

\noindent
Figure \ref{fig:differentiate.stlx} shows a recursive descent parser that implements this grammar.
\begin{enumerate}
\item The function \texttt{parseExpr} recognizes a \texttt{product} in line 2. 
      The value of this \texttt{product} is stored in the variable 
      \texttt{result} together with the list \texttt{Rest} of those tokens that have not been consumed
      yet.  If the list \texttt{Rest} is not empty and the first token in this
      list is either the operator \squoted{+} or the operator \squoted{-},
      then the function \texttt{parseExpr} tries to recognize more products.
      These are added to or subtracted from the \texttt{result} computed so far in
      line 7 or 9.  If there are no more products to be parsed, the \texttt{while} loop 
      terminates and the function returns the \texttt{result} together with the list of the remaining
      tokens \texttt{Rest}.
\item The function \texttt{parseProduct} recognizes a \texttt{factor} in line 13. 
      The value of this \texttt{factor} is stored in the variable 
      \texttt{result} together with the list \texttt{Rest} of those tokens that have not been consumed
      yet.  If the list \texttt{Rest} is not empty and the first token in this
      list is either the operator \squoted{*} or the operator \squoted{/},
      then the function \texttt{parseProduct} tries to recognize more factors.
      The \texttt{result} computed so far is multiplied with or divided by these factors in
      line 18 or 20.  If there are no more products to be parsed, the \texttt{while} loop 
      terminates and the function returns the \texttt{result} together with the list
      \texttt{Rest} of tokens that have not been consumed.
\item The function \texttt{parseFactor} recognizes a \texttt{factor}.
      This is either an expression in parentheses or a number.
      \begin{itemize}
      \item If the first token is a an opening parenthesis, the function tries to parse
            an expression next.  This expression has to be followed by a closing parenthesis.
            The tokens following this closing parenthesis are not consumed but rather are returned 
            together with the result of evaluating the expression.
      \item If the first token is a number, this number is returned together with the list
            of all those tokens that have not been consumed.
      \end{itemize}
\end{enumerate}

\begin{figure}[!ht]
\centering
\begin{minted}[ frame         = lines, 
                framesep      = 0.3cm, 
                firstnumber   = 1,
                bgcolor       = sepia,
                numbers       = left,
                numbersep     = -0.2cm,
                xleftmargin   = 0.3cm,
                xrightmargin  = 0.3cm,
              ]{python3}
    def parseExpr(TL):
        result, Rest = parseProduct(TL)
        while len(Rest) > 1 and Rest[0] in {'+', '-'}: 
            operator = Rest[0]
            arg, Rest = parseProduct(Rest[1:])
            if operator == '+': 
                result += arg
            else:             # operator == '-': 
                result -= arg
        return result, Rest
    
    def parseProduct(TL):
        result, Rest = parseFactor(TL)
        while len(Rest) > 1 and Rest[0] in {'*', '/'}:
            operator = Rest[0]
            arg, Rest = parseFactor(Rest[1:])
            if operator == '*':
                result *= arg
            else:             # operator == '/':
                result /= arg
        return result, Rest
    
    def parseFactor(TL):
        if TL[0] == '(': 
            expr, Rest = parseExpr(TL[1:])
            assert Rest[0] == ')', "ERROR: ')' expected, got {Rest[0]}"
            return expr, Rest[1:]
        else:
            assert isinstance(TL[0], int), "ERROR: Number expected, got {TL[0]}"
            return TL[0], TL[1:]
\end{minted}
\vspace*{-0.3cm}
\caption{A recursive descent parser for the grammar in Figure \ref{fig:arith-expr-ebnf}.}
\label{fig:differentiate.stlx}
\end{figure}

\exerciseEng
In Exercise 21 on page \pageref{exercise:regexp} you have developed an \textsc{Ebnf} grammar for regular
expressions.  Implement a top down parser for this grammar.  The resulting grammar should return
a nested tuple that represents a regular expression.
\eox

\paragraph{Historical Notes} The language \textsc{Algol} \cite{backus:1959,naur:1960} was the first
programming language with a syntax that was based on an \textsc{Ebnf} grammar.  

\section{Check your Understanding}
\begin{enumerate}[(a)]
\item Define the concept of a \blue{context-free grammar}.
\item Assume that $G$ is a context-free grammar.  How is the language $L(G)$ defined?
\item What is the definition of a \blue{parse tree}?
\item How do we transform a parse tree into an \blue{abstract syntax tree}?
\item What are ambiguous grammars?  
\item How can a context free grammar that contains left recursion be transformed into an equivalent grammar
      that does not contain left recursion.
\item How does a top-down parser work? 
\item Why do we have to eliminate left recursion from a grammar in order to build a top-down parser?
\item Define the notion of an \textsc{Ebnf} grammar.
\end{enumerate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "formal-languages.tex"
%%% End: 
