{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open('../style.css').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example has been extracted from the official documentation of Ply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tokenizer for Numbers and the Arithmetical Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module `ply.lex` contains the code that is necessary to create a scanner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a definition of the <em style=\"color:blue\">token names</em>.  Note that all token names have to start with \n",
    "a capital letter.  We have to define these token names as a list with the name `tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\n",
    "   'NUMBER',\n",
    "   'PLUS',\n",
    "   'MINUS',\n",
    "   'TIMES',\n",
    "   'DIVIDE',\n",
    "   'LPAREN',\n",
    "   'RPAREN'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to define these tokens:\n",
    " - *immediate token definitions* define the token by assigning a regular expression to a variable of the form `t_name`,\n",
    "   where `name` is the name of the token that is defined.\n",
    " - *functional token definitions* define the token via a function.  The regular expression that defines the token\n",
    "   is the string appearing in the first line of the function body.\n",
    "   \n",
    "We see examples below.  We start with the *immediate token definitions*.  Note that we have to use *raw strings* here to prevent \n",
    "the expansion of backslash sequences.  Furthermore, those symbols that are interpreted as *operator* symbols inside a regular expression have to be escaped with a backslash character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_PLUS    = r'\\+'\n",
    "t_MINUS   = r'-'\n",
    "t_TIMES   = r'\\*'\n",
    "t_DIVIDE  = r'/'\n",
    "t_LPAREN  = r'\\('\n",
    "t_RPAREN  = r'\\)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to transform the value of a token, we can define the token via a function.  In that case, the first line of the function \n",
    "has to be a string that is a regular expression.  This regular expression then defines the token.  After that,\n",
    "we can add code to transform the token.  The string that makes up the token is stored in `t.value`.  Below, this string\n",
    "is cast into an integer via the predefined function `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_NUMBER(t):\n",
    "    r'0|[1-9][0-9]*'\n",
    "    t.value = int(t.value)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule below is used to keep track of line numbers. We use the function `len` since there might be\n",
    "more than one newline.  The member variable `lexer.lineno` keeps track of the current line number.  This variable\n",
    "is maintained so that we are able to specify the precise location of unkown characters in error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyword `t_ignore` specifies those characters that should be discarded.\n",
    "In the following cell it specifies that space characters and tabulator characters are to be ignored.  Note that we **must not** use a raw string here, since otherwise `\\t` would not denote a tabulator character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ignore = ' \\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All characters not recognized by any of the defined tokens are handled by the function `t_error`.\n",
    "The function `t.lexer.skip(1)` skips one character, which is the character that has not been recognized. Scanning resumes after this character has been discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_error(t):\n",
    "    print(f\"Illegal character '{t.value[0]}' at line {t.lexer.lineno}.\")\n",
    "    print(f\"This is the {t.lexpos}th character.\")\n",
    "    t.lexer.skip(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the function `lex.lex()` creates the lexer specified above.  Since this code is expected to be part \n",
    "of some Python file but really isn't part of a file since it is placed in a Jupyter notebook we have to set the variable \n",
    "`__file__` manually to fool the system into believing that the code given above is located in a file \n",
    "called `hugo.py`.  The name `hugo` is totally irrelevant and could be replaced by any other name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'hugo'\n",
    "lexer = lex.lex(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `lexer` is the scanner that has been created by the previous command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test the generated scanner, that is stored in `lexer`, with the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "       3 + 4 * 10 + 007 + (-20) * 2\n",
    "       42\n",
    "       a\n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us feed the scanner with the string `data`.  This is done by calling the method `input` of the generated scanner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have set the line number to `1` before scanning in order to be able to run the scanner multiple times, since each time the scanner runs the line number is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer.lineno = 1\n",
    "lexer.input(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put the lexer to work by using it as an *iterable*.  This way, we can simply iterate over all the tokens that our scanner recognizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in lexer:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the generated tokens contain four pieces of information:\n",
    " 1. The *type* of the token.\n",
    " 2. The *value* of the token.  This is either a number or a string.\n",
    " 3. The *line number* of the token.  The line number starts with 1.\n",
    "    However, note that the first line of `data` is empty.\n",
    " 4. The *character count*.  For example, the last token is the $54^{\\textrm{th}}$ character.\n",
    "    The character count starts with `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
