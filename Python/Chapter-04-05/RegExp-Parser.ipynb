{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "with open('../style.css', 'r') as file:\n",
    "    css = file.read()\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells loads the `mypy` extension for notebooks.  This enables us to check the type annotation of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Parser for Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a parser for regular expressions. The parser that is implemented in the function `parseExpr` parses a regular expression \n",
    "according to the following <em style=\"color:blue\">EBNF grammar</em>.\n",
    "```\n",
    "   regExp  ‚Üí product ('+' product)*\n",
    "   product ‚Üí factor factor*\n",
    "   factor  ‚Üí atom '*'?\n",
    "   atom    ‚Üí '(' expr ')' | LETTER | 'ùúÄ' | '‚àÖ'\n",
    "```\n",
    "The parse tree is represented as a nested tuple.\n",
    "- letters are represented by themselves,\n",
    "- The character `'‚àÖ'` is interpreted as the regular expression $\\emptyset$ denoting the empty set,\n",
    "- The character `'ùúÄ'` represents the regular expression $\\varepsilon$ denoting the empty string,\n",
    "- $r_1 r_2$ is represented as `(`$r_1$ `, '‚ãÖ',` $r_2$ `)`, \n",
    "- $r_1 + r_2$ is represented as `(`$r_1$ `, '+',` $r_2$ `)`,\n",
    "- $r^*$ is represented as `(` $r$ `, '*')`.\n",
    "\n",
    "The parser is implemented as a recursive *top-down* parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have annotated our function with types, we need to import several items from the module `typing`.  \n",
    "The type `Match` is the type of the object returned by the method `fullmatch` that is used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a definition of the type of the parse trees that are generated.  A parse tree is either\n",
    "* an integer,\n",
    "* a string,\n",
    "* a tuple of parse trees.\n",
    "\n",
    "Hence, this type is a *recursive type*.  First, we define a type variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParseTree = TypeVar('ParseTree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we give the recursive definition of this type variable.\n",
    "The *ellipsis* `...` specifies that the tuple can be of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParseTree = int | str | tuple[ParseTree, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tokenize strings, we need regular expressions from the module `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `tokenize(s)` partitions the string `s` into a list of tokens.\n",
    "It recognizes \n",
    "- the operator symbols `+` and `*`, \n",
    "- the parentheses `(`, `)`, \n",
    "- single upper or lower case letters, \n",
    "- `0`, \n",
    "- the empty string `\"\"`.\n",
    "\n",
    "All whitespace characters (and, indeed, all characters that could not be matched) are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s: str) -> list[str]:\n",
    "    regExp = r'''\n",
    "              [+*()]   |  # operators and parentheses\n",
    "              [a-zA-Z] |  # single characters from the alphabet\n",
    "              ‚àÖ        |  # empty regular expression\n",
    "              ùúÄ           # epsilon\n",
    "              '''\n",
    "    return [t for t in re.findall(regExp, s, flags=re.VERBOSE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('a*bc + ba*c + (ùúÄ+c*) + ‚àÖ*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have defined forward declarations of some functions that are used later. \n",
    "This is necessary, since these functions are mutually recursive.  \n",
    "\n",
    "As these are only stubs, there is no need to type check their body.  \n",
    "Therefore, we switch of the type checking for the return statement.  \n",
    "This is done via the *pragma* `# type: ignore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRegExp(TokenList: list[str])  -> tuple[ParseTree, list[str]]: \n",
    "    return None # type: ignore\n",
    "\n",
    "def parseProduct(TokenList: list[str]) -> tuple[ParseTree, list[str]]: \n",
    "    return None # type: ignore\n",
    "\n",
    "def parseFactor(TokenList: list[str])  -> tuple[ParseTree, list[str]]: \n",
    "    return None # type: ignore\n",
    "\n",
    "def parseAtom(TokenList: list[str]) -> tuple[str | int | ParseTree, list[str]]:\n",
    "    return None # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parse` takes a string `s` and tries to parse it as a regular expression.  \n",
    "It returns the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(s: str) -> ParseTree:\n",
    "    TokenList = tokenize(s)\n",
    "    regExp, Rest = parseRegExp(TokenList)\n",
    "    assert Rest == [], f'Parse Error: could not parse {TokenList}'\n",
    "    return regExp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parseRegExp` takes a token list `TokenList` and tries to interpret this list as a regular expression.  \n",
    "It returns the regular expression in the form of a nested tuple and a list of those tokens that could not be parsed.  \n",
    "It is implemented as a <em style=\"color:blue\">top-down-parser.</em> \n",
    "\n",
    "The function `parseRegExp` implements the following grammar rule:\n",
    "```\n",
    "regExp ‚Üí product ('+' product)*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRegExp(TokenList: list[str]) -> tuple[ParseTree, list[str]]:\n",
    "    result, Rest = parseProduct(TokenList)\n",
    "    while len(Rest) >= 2 and Rest[0] == '+':\n",
    "        arg, Rest = parseProduct(Rest[1:])\n",
    "        result = (result, '+', arg)\n",
    "    return result, Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parseProduct` implements the following grammar rule:\n",
    "```\n",
    "product ‚Üí factor factor*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseProduct(TokenList: list[str]) -> tuple[ParseTree, list[str]]:\n",
    "    result, Rest = parseFactor(TokenList)\n",
    "    while len(Rest) > 0 and Rest[0] in string.ascii_letters + '(':\n",
    "        arg, Rest = parseFactor(Rest)\n",
    "        result = (result, '‚ãÖ', arg)\n",
    "    return result, Rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parseFactor` implements the following grammar rule:\n",
    "```\n",
    "factor ‚Üí atom '*'?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFactor(TokenList: list[str]) -> tuple[ParseTree, list[str]]:\n",
    "    atom, Rest = parseAtom(TokenList)\n",
    "    if len(Rest) > 0 and Rest[0] == \"*\":\n",
    "        return (atom, '*'), Rest[1:]\n",
    "    return atom, Rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parseAtom` implements the following grammar rule:\n",
    "```\n",
    "atom  ‚Üí '(' expr ')'\n",
    "      | '‚àÖ'            # denotes empty set ‚àÖ\n",
    "      | 'ùúÄ'            # denotes empty string ùúÄ\n",
    "      | LETTER         # any letter denotes itself\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAtom(TokenList: list[str]) -> tuple[str | int | ParseTree, list[str]]:\n",
    "    if TokenList[0] == '(':\n",
    "        regExp, Rest = parseRegExp(TokenList[1:])\n",
    "        assert Rest[0] == ')', \"Parse Error\"\n",
    "        return regExp, Rest[1:]\n",
    "    if TokenList[0] == '‚àÖ':\n",
    "        return 0, TokenList[1:]\n",
    "    if TokenList[0] == 'ùúÄ':\n",
    "        return 'ùúÄ', TokenList[1:]\n",
    "    s = TokenList[0]\n",
    "    if s in string.ascii_letters:\n",
    "        return s, TokenList[1:]\n",
    "    assert False, f'parse error: {TokenList}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse('a*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse('ab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse('a+b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse('ab+cd*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse('(a+b*)*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
